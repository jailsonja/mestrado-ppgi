{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração Sentenças e Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from ast import literal_eval\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diretorios dos datasets\n",
    "filenames = ['cameras.xml','cells.xml','dvds.xml','laptops.xml','routers.xml']\n",
    "\n",
    "# files gabaritos\n",
    "gabaritos = ['DadosCamera.txt', 'DadosCells.txt', 'DadosDvds.txt', 'DadosLaptops.txt', 'DadosRouters.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diretorios dos arquivos das sentencas\n",
    "save_sentencas = ['sentencas-camera.txt','sentencas-cells.txt','sentencas-dvds.txt','sentencas-laptops.txt','sentencas-routers.txt']\n",
    "\n",
    "#Diretorios dos arquivos de palavras-sentencas\n",
    "save_palavras = ['palavras-camera.json','palavras-cells.json','palavras-dvds.json','palavras-laptops.json','palavras-routers.json']\n",
    "\n",
    "#Diretorios dos arquivos de frequencia\n",
    "save_frequencias = ['frequencia-camera.json','frequencia-cells.json','frequencia-dvds.json','frequencia-laptops.json','frequencia-routers.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função que retorna a sentenca\n",
    "def get_Sentence(s):\n",
    "    resp = \"\"\n",
    "    for i in range(len(s)):\n",
    "        if(s[i] == '>'):\n",
    "            resp = s[i+1:]\n",
    "            break\n",
    "    resp = resp.strip()\n",
    "    return resp\n",
    "\n",
    "def index_in_list(a_list, index):\n",
    "    return len(a_list) > index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_products(filename):\n",
    "    arq = open('../../datasets/gabaritos/' + filename, 'r')\n",
    "    cla = []\n",
    "    for line in arq.readlines():\n",
    "        idx = line.index(':')\n",
    "        cla.append(line[:idx])\n",
    "    arq.close()\n",
    "    \n",
    "    return cla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = ['cameras', 'cells', 'dvds', 'laptops', 'routers']\n",
    "uknow = {}\n",
    "for value in products:\n",
    "    uknow[value] = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Palavras e Setencas: 100%|██████████| 18/18 [00:00<00:00, 226040.34it/s]\n",
      "Sentencas: 100%|██████████| 18/18 [00:00<00:00, 68138.51it/s]\n",
      "Frequencia Palavra: 100%|██████████| 18/18 [00:00<00:00, 8136.38it/s]\n",
      "Palavras e Setencas: 100%|██████████| 20/20 [00:00<00:00, 206108.30it/s]\n",
      "Sentencas: 100%|██████████| 20/20 [00:00<00:00, 50625.27it/s]\n",
      "Frequencia Palavra: 100%|██████████| 20/20 [00:00<00:00, 4196.61it/s]\n",
      "Palavras e Setencas: 100%|██████████| 12/12 [00:00<00:00, 200524.49it/s]\n",
      "Sentencas: 100%|██████████| 12/12 [00:00<00:00, 58730.04it/s]\n",
      "Frequencia Palavra: 100%|██████████| 12/12 [00:00<00:00, 10072.37it/s]\n",
      "Palavras e Setencas: 100%|██████████| 24/24 [00:00<00:00, 238538.62it/s]\n",
      "Sentencas: 100%|██████████| 24/24 [00:00<00:00, 59918.63it/s]\n",
      "Frequencia Palavra: 100%|██████████| 24/24 [00:00<00:00, 3549.36it/s]\n",
      "Palavras e Setencas: 100%|██████████| 20/20 [00:00<00:00, 288268.32it/s]\n",
      "Sentencas: 100%|██████████| 20/20 [00:00<00:00, 106050.67it/s]\n",
      "Frequencia Palavra: 100%|██████████| 20/20 [00:00<00:00, 10945.47it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, filename in enumerate(filenames):\n",
    "    produto = filename.replace('.xml', '')\n",
    "    class_product = get_class_products(gabaritos[i])\n",
    "    dataset_product = {}\n",
    "    for cla in class_product:\n",
    "        dataset_product[cla] = defaultdict(list)\n",
    "    \n",
    "    sentencas = [] #Variavel que guarda as sentencas do documento\n",
    "    sentencas_not_class = {} #Variavel que guarda os documentos do dominio\n",
    "    lista_index = []\n",
    "    \n",
    "    arq = open('../../datasets/' + filename,'r')\n",
    "    for j, linha in enumerate(arq.readlines()):\n",
    "        if('<sentence' in linha):\n",
    "            s = linha.replace('</sentence>','')\n",
    "            s = s.split('idSentence=')[1]\n",
    "            s = get_Sentence(s) #Obtendo a sentenca\n",
    "            sentencas.append(s)\n",
    "            sentencas_not_class[len(sentencas) - 1] = s\n",
    "            \n",
    "        if('<opinion>' in linha):\n",
    "            \n",
    "            p = linha.replace('/opinion','opinion')\n",
    "            p = p.replace('<opinion>','')\n",
    "            p = p.split('\"')\n",
    "            aspect_class = p[7].lower()\n",
    "            palavra = p[1].lower() #Obtendo o opinion (atributo)\n",
    "            tipo = p[-2]\n",
    "            if(tipo != 'anaphora' and (palavra not in set(stopwords.words('english')))):\n",
    "                if(aspect_class not in dataset_product.keys()):\n",
    "                    dataset_product[aspect_class] = defaultdict(list)\n",
    "                    \n",
    "                dataset_product[aspect_class][palavra].append(sentencas[len(sentencas) - 1])\n",
    "                lista_index.append(len(sentencas) - 1)\n",
    "                \n",
    "        if('</review>' in linha):\n",
    "            lista_set = set(lista_index)\n",
    "                  \n",
    "            for key in sentencas_not_class.keys():\n",
    "                if key in lista_set:\n",
    "                    continue\n",
    "                else:\n",
    "                    uknow[produto].append(sentencas[key])     \n",
    "            \n",
    "            lista_index = []\n",
    "            sentencas = []\n",
    "            sentencas_not_class = {}\n",
    "                \n",
    "    arq.close()\n",
    "    \n",
    "    #Salvando as palavras com suas sentencas nos arquivos\n",
    "    data_words = {}\n",
    "    with open('../../datasets_processed/' + save_palavras[i],'w') as fp:\n",
    "        for key in tqdm(dataset_product.keys(), desc='Palavras e Setencas'):\n",
    "            for value in sorted(dataset_product[key].keys()):\n",
    "                data_words[value] = dataset_product[key][value]\n",
    "        \n",
    "        json.dump(data_words, fp)\n",
    "    \n",
    "    \n",
    "    #salvando sentenças em arquivos\n",
    "    arq = open('../../datasets_processed/sentencas/' + save_sentencas[i],'w')\n",
    "    for key in tqdm(dataset_product.keys(), desc='Sentencas'):\n",
    "        for value in sorted(dataset_product[key].keys()):\n",
    "            for sent in dataset_product[key][value]:\n",
    "                arq.write(sent + '\\n')\n",
    "    arq.close()\n",
    "    \n",
    "    # lendo sentencas\n",
    "    arq = open('../../datasets_processed/sentencas/' + save_sentencas[i],'r')\n",
    "    texto = arq.read().lower() #Obtem todos os documentos\n",
    "    arq.close()\n",
    "    \n",
    "    # salvando frequecia de palavras\n",
    "    data_freq = {}\n",
    "    with open('../../datasets_processed/frequencias/' + save_frequencias[i],'w') as fp:\n",
    "        for key in tqdm(dataset_product.keys(), desc='Frequencia Palavra'):\n",
    "            for value in sorted(dataset_product[key].keys()):\n",
    "                frequencia = 0\n",
    "                frequencia = texto.count(value)\n",
    "                data_freq[value] = str(frequencia)\n",
    "        \n",
    "        json.dump(data_freq, fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_product.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
