{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo que está sendo utilizado atualmente\n",
    "\n",
    "Este algoritmo é uma variação do algoritmo descrito no primeiro paper, que utiliza matrizes de similaridade e de contexto para realizar os agrupamentos. Este algoritmo utiliza a média ponderada do valor absoluto da similaridade com o valor do contexto entre as palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division  # Biblioteca que torna a divisão decimal\n",
    "from requests import get  # Biblioteca responsavel pelas requisições a Internet\n",
    "import time\n",
    "import math\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import ast\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As funções definidas e utilizadas no algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função que retorna o cosseno entre dois vetores de linha u e v\n",
    "def cosseno(u, v):\n",
    "    #return np.dot(u,v)/math.sqrt(np.dot(u,u)*np.dot(v,v))\n",
    "    return np.dot(u, v)/(np.linalg.norm(u)*np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função que retorna a similaridade em relação a matriz G\n",
    "def simG(x, y, G, indices):\n",
    "    i = indices[x]\n",
    "    j = indices[y]\n",
    "    return cosseno(G[i], G[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função que retorna a similaridade em relação a matriz T\n",
    "def simT(x, y, T, indices):\n",
    "    i = indices[x]\n",
    "    j = indices[y]\n",
    "    return cosseno(T[i], T[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função que retorna a similaridade em relação a matriz G e T (coocorrências de duas palavras)\n",
    "def simGT(x, y, G, T, indices):\n",
    "    i = indices[x]\n",
    "    j = indices[y]\n",
    "    return max(cosseno(G[i], T[j]), cosseno(T[i], G[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função que retorna a similaridade entre duas palavras\n",
    "def sim(x, y, G, T, indices):\n",
    "    wg = 0.2\n",
    "    wt = 0.2\n",
    "    wgt = 0.6\n",
    "    v = (wg*simG(x, y, G, indices)) + (wt*simT(x, y, T, indices)) + (wgt*simGT(x, y, G, T, indices))\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função que retorna a similaridade baseado nas matrizes de similaridade e contexto\n",
    "def similarity(x,y,G,T,indices):\n",
    "    wg = 0.5\n",
    "    wt = 0.5\n",
    "    p1 = indices[x] #Obtem-se o indice da palavra 1\n",
    "    p2 = indices[y] #Obtem-se o indice da palavra 2\n",
    "    v = (wg*G[p1][p2]) + (wt*T[p1][p2]) #Obtem-se a similaridade e o contexto das palavras\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função que retorna a similaridade entre dois termos usando a ideia do paper maas como uma adaptação\n",
    "def similarityAdaptada(x, y, G, T, indices):\n",
    "    wg = 0.5\n",
    "    wt = 0.5\n",
    "    v = (wg*simG(x, y, G, indices)) + (wt*simT(x, y, T, indices))\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função que retorna a distância média em relação a similaridade dos termos dos Clusteres\n",
    "def distAvg(C1, C2, G, T, indices):\n",
    "    tam1 = len(C1)\n",
    "    tam2 = len(C2)\n",
    "    soma = 0\n",
    "    for el in C1:\n",
    "        for val in C2:\n",
    "            #soma += 1 - sim(el, val, G, T, indices) #Obtem-se a distancia entre as palavras\n",
    "            soma += 1 - similarity(el,val,G,T,indices)\n",
    "            #soma += 1 - similarityAdaptada(el, val, G, T, indices) #Obtem-se a distancia entre as palavras\n",
    "    return soma/(tam1*tam2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função que retorna a distância representativa entre os clusteres em função da\n",
    "#similaridade dos representantes, os que possuem maior frequência\n",
    "def distRep(C1, C2, G, T, indices):\n",
    "    rep1 = C1[0]\n",
    "    rep2 = C2[0]\n",
    "    #ans = 1 - sim(rep1, rep2, G, T, indices) #Obtem-se a distancia entre os termos\n",
    "    ans = 1 - similarity(rep1, rep2, G, T, indices)\n",
    "    #ans = 1 - similarityAdaptada(rep1, rep2, G, T, indices) #Obtem-se a distancia entre os termos\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função que verifica se dois clusteres são mesclaveis\n",
    "def checagem(C1,C2,mapeamentos):\n",
    "    x = 0 #Variavel que guarda o numero de coocorrencias em um arquivo\n",
    "    y = 0 #Variavel que guarda o numero de sentencas em que ambas nao ocorrem\n",
    "    n = len(mapeamentos[C1[0]]) #Quantidade de documentos\n",
    "    for el in C1:\n",
    "        for val in C2:\n",
    "            #Percorrendo cada documento\n",
    "            for i in range(n):\n",
    "                d1 = mapeamentos[el][i] #Obtendo as sentencas que 'el' aparece no documento atual\n",
    "                d2 = mapeamentos[val][i] #Obtendo as sentencas que 'val' aparece no documento atual\n",
    "                #Se ambos aparecem no documento, então\n",
    "                if(len(d1) > 0 and len(d2) > 0):\n",
    "                    inter = d1.intersection(d2) #Obtem-se as sentencas em comum a ambos\n",
    "                    union = d1.union(d2) #Obtem-se a união de ambas\n",
    "                    x += len(inter) #Quantidade de sentencas em comum\n",
    "                    y += len(union - inter) #Quantidade de sentencas incomum\n",
    "    return x > y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função que retorna se os clusteres devem ser juntados ou não\n",
    "def violateConstraints(C1, C2, limiar, G, T, indices,mapeamentos):\n",
    "    metrica3 = False\n",
    "    #Executando a primeira condicao de mesclagem do artigo\n",
    "    dm = distAvg(C1, C2, G, T, indices) #Distancia média entre os clusteres\n",
    "    dr = distRep(C1, C2, G, T, indices) #Distancia entre os representantes dos clusteres\n",
    "    distancia = max(dm, dr) #Distancia maxima\n",
    "    metrica1 = (distancia >= limiar) #Verifica se viola a regra de distancia\n",
    "    metrica2 = (not checagem(C1,C2,mapeamentos)) #Verifica se viola a metrica 3\n",
    "    #Executando a segunda condicao de mesclagem do artigo\n",
    "    for el in C1:\n",
    "        if(' ' not in el):\n",
    "            tupla = TextBlob(el).pos_tags[0]\n",
    "            if(\"JJ\" in tupla[1] or \"VB\" in tupla[1]):\n",
    "                metrica3 = True\n",
    "                break\n",
    "    if(not metrica3):\n",
    "        for val in C2:\n",
    "            if(' ' not in val):\n",
    "                tupla = TextBlob(val).pos_tags[0]\n",
    "                if(\"JJ\" in tupla[1] or \"VB\" in tupla[1]):\n",
    "                    metrica3 = True\n",
    "                    break\n",
    "    return (metrica1 or metrica2 or metrica3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatriz(source):\n",
    "    matriz = []\n",
    "    with open('../../datasets_processed/matrizes/' + source, 'r') as fp:\n",
    "        data = json.load(fp)\n",
    "        \n",
    "    for _, value in data.items():\n",
    "        matriz.append(value)\n",
    "    return matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função responsável por mesclar os clusteres - Etapa 1 do Algoritmo\n",
    "def mesclar(groups, limiar, G, T, indices,mapeamentos):\n",
    "    i = 0\n",
    "    while(i < len(groups)):\n",
    "        j = 0\n",
    "        while(j < len(groups)):\n",
    "            if(i != j):\n",
    "                metrica1 = not violateConstraints(groups[i], groups[j], limiar, G, T, indices,mapeamentos)\n",
    "                if(metrica1):\n",
    "                    #Se os clusteres são mesclaveis\n",
    "                    if(j > i):\n",
    "                        groups[i].extend(groups[j]) #Unem-se os clusteres\n",
    "                        groups.remove(groups[j]) #Remove um dos clusteres do geral\n",
    "                    else:\n",
    "                        groups[j].extend(groups[i])\n",
    "                        groups.remove(groups[i])\n",
    "                else:\n",
    "                    #Caso contrario verifica-se o proximo\n",
    "                    j += 1\n",
    "            else:\n",
    "                j += 1\n",
    "        i += 1\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função responsável por adicionar os elementos restantes a um cluster ou o transformar em um\n",
    "#novo cluster - Etapa 2 do Algoritmo\n",
    "def mapear(elemento, groups, limiar, G, T, indices,mapeamentos):\n",
    "    novo = True\n",
    "    for i in range(len(groups)):\n",
    "        metrica1 = not violateConstraints(groups[i], [elemento], limiar, G, T, indices,mapeamentos)\n",
    "        if(metrica1):\n",
    "            #Se os clusteres forem mesclaveis\n",
    "            groups[i].extend([elemento]) #Une-se o elemento ao cluster\n",
    "            novo = False\n",
    "    if(novo):\n",
    "        #Se o elemento não foi mesclado com ninguem\n",
    "        groups.append([elemento]) #Assume-se que este elemento é um novo cluster\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função que retorna o numero de grupos desejado\n",
    "def select(frequencias, groups):\n",
    "    somatorio = {} #Mapa dos clusteres baseado no somatorio das frequencias\n",
    "    somas = []\n",
    "    resp = [] #Variavel com a resposta\n",
    "    ind = 0\n",
    "    #Mapeamento dos grupos baseado na soma das frequencias dos elementos\n",
    "    for g in groups:\n",
    "        soma = 0\n",
    "        for v in g:\n",
    "            soma += frequencias[v]\n",
    "        if(soma not in somatorio.keys()):\n",
    "            somatorio[soma] = []\n",
    "        somatorio[soma].append(ind)\n",
    "        ind += 1\n",
    "    #Ordena-se os somatorios das frequencias em ordem decrescente\n",
    "    for s in sorted(somatorio.keys(),reverse=True):\n",
    "        index = somatorio[s]\n",
    "        for i in index:\n",
    "            #Armazena-se os grupos de maior frequencia na variavel de resposta\n",
    "            resp.append(groups[i])\n",
    "            somas.append(s)\n",
    "    return resp,somas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencias = ['frequencia-camera.json', 'frequencia-cells.json', 'frequencia-dvds.json', 'frequencia-laptops.json', 'frequencia-routers.json']\n",
    "\n",
    "agrupamentos = ['agrupamentos-finais-camera.json','agrupamentos-finais-cells.json', 'agrupamentos-finais-dvds.json', 'agrupamentos-finais-laptops.json', 'agrupamentos-finais-routers.json']\n",
    "\n",
    "saves_gabarito = ['DadosCamera.txt', 'DadosCells.txt', 'DadosDvds.txt', 'DadosLaptops.txt','DadosRouters.txt']\n",
    "\n",
    "similaridades = ['similaridades-camera.json', 'similaridades-cells.json', 'similaridades-dvds.json', 'similaridades-laptops.json', 'similaridades-routers.json']\n",
    "\n",
    "contextos = ['contexto-camera.json','contexto-cells.json','contexto-dvds.json','contexto-laptops.json','contexto-routers.json']\n",
    "\n",
    "contextualizacao = ['termo-documento-camera.json', 'termo-documento-cells.json', 'termo-documento-dvds.json', 'termo-documento-laptops.json', 'termo-documento-routers.json']\n",
    "\n",
    "contexts = ['contextualizacao-camera.json', 'contextualizacao-cells.json', 'contextualizacao-dvds.json', 'contextualizacao-laptops.json', 'contextualizacao-routers.json']\n",
    "\n",
    "k = [9,10,6,12,10] #Numero de grupos por dominio\n",
    "k1 = [7,8,4,10,8] #NUmero de grupos sem 'General' e 'Others'\n",
    "s = 25 #Numero de clusteres iniciais\n",
    "limiar = 0.5 #Valor de limiar para as distancias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sementes = ['sementes-camera.json','sementes-cells.json','sementes-dvds.json','sementes-laptops.json','sementes-routers.json']\n",
    "\n",
    "sementes_alternativo = ['sementes-camera-alternativo.json','sementes-cells-alternativo.json','sementes-dvds-alternativo.json','sementes-laptops-alternativo.json','sementes-routers-alternativo.json']\n",
    "\n",
    "iniciais = ['clusteres-iniciais-camera.json', 'clusteres-iniciais-cells.json', 'clusteres-iniciais-dvds.json', 'clusteres-iniciais-laptops.json', 'clusteres-iniciais-routers.json']\n",
    "\n",
    "iniciais_alternativo = ['clusteres-iniciais-camera-alternativo.json', 'clusteres-iniciais-cells-alternativo.json', 'clusteres-iniciais-dvds-alternativo.json', 'clusteres-iniciais-laptops-alternativo.json', 'clusteres-iniciais-routers-alternativo.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusteres_finais = ['clusteres-finais-camera.json', 'clusteres-finais-cells.json', 'clusteres-finais-dvds.json', 'clusteres-finais-laptops.json', 'clusteres-finais-routers.json']\n",
    "\n",
    "clusteres_finais_alternativo = ['clusteres-finais-camera-alternativo.json', 'clusteres-finais-cells-alternativo.json', 'clusteres-finais-dvds-alternativo.json', 'clusteres-finais-laptops-alternativo.json', 'clusteres-finais-routers-alternativo.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "agrupamentos_alternativo = ['agrupamentos-finais-camera-alternativo.txt', 'agrupamentos-finais-cells-alternativo.txt', 'agrupamentos-finais-dvds-alternativo.txt', 'agrupamentos-finais-laptops-alternativo.txt', 'agrupamentos-finais-routers-alternativo.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusteres = ['clusteres-camera.json', 'clusteres-cells.json', 'clusteres-dvds.json', 'clusteres-laptops.json', 'clusteres-routers.json']\n",
    "\n",
    "clusteres_alternativo = ['clusteres-camera-alternativo.json', 'clusteres-cells-alternativo.json', 'clusteres-dvds-alternativo.json', 'clusteres-laptops-alternativo.json', 'clusteres-routers-alternativo.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../datasets_processed/contextualizacao/termo-documento-camera.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-ca3e85fc794a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# obtendo os mapeamentos de term-document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../datasets_processed/contextualizacao/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcontextualizacao\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mdata_ctx_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../datasets_processed/contextualizacao/termo-documento-camera.json'"
     ]
    }
   ],
   "source": [
    "for idx, ctx in enumerate(contexts):\n",
    "    freq_atribute = {} #variavel que mapeia atributo para frequencia\n",
    "    freq_valor = {} #variavel que mapeia frequencias para atributos\n",
    "    grupos = [] #Variavel que guardará os grupos formados\n",
    "    resto = [] #palavras que restaram após a primeira etapa\n",
    "    matrizG = [] #Matriz de similaridade\n",
    "    matrizT = [] #Matriz de contexto\n",
    "    indices = {} #Variavel que mapeia os atributos para os indices\n",
    "    gabarito = {} #Variavel que guarda o gabarito especificado\n",
    "    mapeamentos = {} #Variavel que guarda o mapeamento de palavras-sentencas\n",
    "    \n",
    "    #Inicializando as variaveis de acordo com o dominio especificado\n",
    "    #saves_gabarito_atual = saves_gabarito[dominio]\n",
    "    contexts_atual = contexts[idx]\n",
    "    frequencias_atual = frequencias[idx]\n",
    "    \n",
    "    #Inicializando as variaveis de acordo com o dominio especificado\n",
    "    similaridades_atual = similaridades[idx]\n",
    "    contextos_atual = contextos[idx]\n",
    "    contextualizacao_atual = contextualizacao[idx]\n",
    "    \n",
    "    #Inicializando as variaveis de acordo com o dominio especificado\n",
    "    sementes_atual = sementes_alternativo[idx]\n",
    "    iniciais_atual = iniciais_alternativo[idx]\n",
    "    \n",
    "    #Inicializando as variaveis de acordo com o dominio especificado\n",
    "    clusteres_finais_atual = clusteres_finais_alternativo[idx]\n",
    "    \n",
    "    #Inicializando as variaveis de acordo com o dominio especificado\n",
    "    agrupamentos_atual = agrupamentos_alternativo[idx]\n",
    "    \n",
    "    #Inicializando as variaveis de acordo com o dominio especificado\n",
    "    clusteres_atual = clusteres_alternativo[idx]\n",
    "    \n",
    "    # obtendo os mapeamentos de term-document\n",
    "    with open('../../datasets_processed/contextualizacao/' + contextualizacao[idx], 'r') as fp:\n",
    "        data_ctx_a = json.load(fp)\n",
    "        \n",
    "    for key, value in data_ctx_a.items():\n",
    "        mapeamentos[key] = [set(v) for v in value]\n",
    "        \n",
    "    with open('../../datasets_processed/frequencias/' + frequencias[idx], 'r') as fp:\n",
    "        data_freq = json.load(fp)\n",
    "        \n",
    "    for key in sorted(data_freq):\n",
    "        freq_atribute[key] = data_freq[key]\n",
    "        \n",
    "        if(data_freq[key] not in freq_valor.keys()):\n",
    "            freq_valor[data_freq[key]] = []\n",
    "        \n",
    "        freq_valor[data_freq[key]].append(key)\n",
    "        \n",
    "    cont = 0\n",
    "    for key in sorted(freq_atribute.keys()):\n",
    "        indices[key] = cont\n",
    "        cont += 1\n",
    "        \n",
    "    matrizG = getMatriz(similaridade_atual)\n",
    "    matrizT = getMatriz(contextos_atual)\n",
    "    \n",
    "    # Primeira etapa do algoritmo\n",
    "    for val in sorted(freq_valor.keys(), reverse=True):\n",
    "        for item in sorted(freq_valor[val]):\n",
    "            if(len(grupos) < s):\n",
    "                grupos.append([item])\n",
    "            else:\n",
    "                # as palavras restantes são armazenadas em uma variavel\n",
    "                resto.append(item)\n",
    "                \n",
    "    print(ctx.split('-')[1].replace(\".json\", \"\") + \">> Número de Clusteres:{}\".format(len(grupos)))\n",
    "    print(ctx.split('-')[1].replace(\".json\", \"\") + \" Grupos: {}\".format(grupos))\n",
    "    print()\n",
    "    \n",
    "    with open('../../resultados/sementes/' + sementes_atual, 'w') as fp:\n",
    "        data_sem_atual = {}\n",
    "        for i, g in enumerate(grupos):\n",
    "            data_sem_atual[i] = g\n",
    "        \n",
    "        json.dump(data_sem_atual, fp)\n",
    "            \n",
    "    # SEgunda Etapa do Algoritmo\n",
    "    print('Mesclando os Clusteres iniciais...')\n",
    "    \n",
    "    #mesclagem dos s clusteres iniciais de maior frequencia\n",
    "    grupos = mesclar(grupos, limiar, matrizG, matrizT, indices, mapeamentos)\n",
    "    \n",
    "    # salvando o resultado da primeira etapa em um arquivo\n",
    "    with open('../../resultados/clusteres/' + iniciais_atual, 'w') as fp:\n",
    "        data_ini_atual = {}\n",
    "        for i, g in enumerate(grupos):\n",
    "            data_ini_atual[i] = g\n",
    "        \n",
    "        json.dump(data_ini_atual, fp)\n",
    "        \n",
    "    print('Mesclando palavras restantes...')\n",
    "    for palavra in resto:\n",
    "        grupos = mapear(palavra, grupos, limiar, matrizG, matrizT, indices, mapeamentos)\n",
    "        \n",
    "    # salvando o resultado da segunda etapa em um arquivo\n",
    "    with open('../../resultados/clusteres/' + clusteres_finais_atual, 'w') as fp:\n",
    "        data_clt_final = {}\n",
    "        for i, g in enumerate(grupos):\n",
    "            data_clt_final[i] = g\n",
    "        json.dump(data_clt_final, fp)   \n",
    "    # Terceira Etapa do Algortimo\n",
    "    resposta, sums = select(freq_atribute, grupos)\n",
    "    \n",
    "    print('Clusteres finais')\n",
    "    \n",
    "    # Armazenando os agrupamentos fianis em um arquivo\n",
    "    with open('../../resultados/agrupamentos/' + agrupamentos_atual, 'w') as fp:\n",
    "        data_agr_atual = {}\n",
    "        for i, x in enumerate(resposta):\n",
    "            data_agr_atual[i] = x\n",
    "        json.dump(data_agr_atual, fp)\n",
    "        \n",
    "    #Etapa Incremental\n",
    "    outros = []\n",
    "    ans = []\n",
    "    \n",
    "    for g in resposta:\n",
    "        if(len(g) > 2):\n",
    "            ans.append(g)\n",
    "        else:\n",
    "            outros.append(g)\n",
    "            \n",
    "    print(\"Mesclando Outros\")\n",
    "    outros = mesclar(outros, limiar, matrizG, matrizT, indices, mapeamentos)\n",
    "    \n",
    "    resp = ans + outros\n",
    "    resp = mesclar(resp, limiar, matrizG, matrizT, indices, mapeamentos)\n",
    "    resp, sums = select(freq_atribute, resp)\n",
    "    \n",
    "    with open('../../resultados/clusteres/' + clusteres_atual, 'w') as fp:\n",
    "        data_grp = {}\n",
    "        for i, a in enumerate(ans):\n",
    "            data_grp[i] = a\n",
    "        json.dump(data_grp, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camera\n",
      "cells\n",
      "dvds\n",
      "laptops\n",
      "routers\n"
     ]
    }
   ],
   "source": [
    "for idx, ctx in enumerate(contexts):\n",
    "    print(ctx.split('-')[1].replace(\".json\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
